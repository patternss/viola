{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc656ddb-c6fd-4cc3-8b6f-3e59844669c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importattu\n",
      "vector store ja retriever rdy\n",
      "starting...\n"
     ]
    },
    {
     "ename": "ResponseError",
     "evalue": "model requires more system memory (5.6 GiB) than is available (5.5 GiB) (status code: 500)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mResponseError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 83\u001b[39m\n\u001b[32m     69\u001b[39m pedagogical_prompt = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mKäyttäen mahdollista RAG kontekstia alla sekä omia tietojasi, \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[32m     70\u001b[39m \u001b[33mja vastaa näiden pohjalta käyttäjän kysymykseen.\u001b[39m\u001b[33m\"\u001b[39m \n\u001b[32m     73\u001b[39m router_message = \u001b[33m'\u001b[39m\u001b[33mSinun tehtäväsi on valita miten ohjelmaa suoritetaan seuraavaksi käyttäjäviestin \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[32m     74\u001b[39m \u001b[33mperusteella. Jos viestistä löytyy seuraavia sanoja \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mlähde, lähdeaineisto, viimeaikainen, uusi, \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[32m     75\u001b[39m \u001b[33metsiä, etsi, viittaus, viitaten\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m, tai jos vaikuttaa siltä, että käyttäjä etsii jotain spesifiä \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m \u001b[33mmuodossa: \u001b[39m\u001b[33m{\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mroute\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m:\u001b[39m\u001b[33m\"\u001b[39m\u001b[33msuora |epäsuora\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mreason\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m:\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmiksi valitsit tämän reitin\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRAG\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m : \u001b[39m\u001b[33m\"\u001b[39m\u001b[33myes | no\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m} \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[32m     81\u001b[39m \u001b[33mPalauta VAIN JSON-yksiö ilman selityksiä, ilman koodiaitoja, ilman ylimääräisiä rivinvaihtoja:\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m resp = ollama.chat(model=model, messages=[{\u001b[33m'\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m: beginning_preprompt}])\n\u001b[32m     84\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mViola: \u001b[39m\u001b[33m'\u001b[39m, resp[\u001b[33m'\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     86\u001b[39m \u001b[38;5;66;03m# Conversation loop\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\ollama\\_client.py:342\u001b[39m, in \u001b[36mClient.chat\u001b[39m\u001b[34m(self, model, messages, tools, stream, think, format, options, keep_alive)\u001b[39m\n\u001b[32m    297\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mchat\u001b[39m(\n\u001b[32m    298\u001b[39m   \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    299\u001b[39m   model: \u001b[38;5;28mstr\u001b[39m = \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    307\u001b[39m   keep_alive: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    308\u001b[39m ) -> Union[ChatResponse, Iterator[ChatResponse]]:\n\u001b[32m    309\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    310\u001b[39m \u001b[33;03m  Create a chat response using the requested model.\u001b[39;00m\n\u001b[32m    311\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    340\u001b[39m \u001b[33;03m  Returns `ChatResponse` if `stream` is `False`, otherwise returns a `ChatResponse` generator.\u001b[39;00m\n\u001b[32m    341\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m342\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._request(\n\u001b[32m    343\u001b[39m     ChatResponse,\n\u001b[32m    344\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mPOST\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    345\u001b[39m     \u001b[33m'\u001b[39m\u001b[33m/api/chat\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    346\u001b[39m     json=ChatRequest(\n\u001b[32m    347\u001b[39m       model=model,\n\u001b[32m    348\u001b[39m       messages=\u001b[38;5;28mlist\u001b[39m(_copy_messages(messages)),\n\u001b[32m    349\u001b[39m       tools=\u001b[38;5;28mlist\u001b[39m(_copy_tools(tools)),\n\u001b[32m    350\u001b[39m       stream=stream,\n\u001b[32m    351\u001b[39m       think=think,\n\u001b[32m    352\u001b[39m       \u001b[38;5;28mformat\u001b[39m=\u001b[38;5;28mformat\u001b[39m,\n\u001b[32m    353\u001b[39m       options=options,\n\u001b[32m    354\u001b[39m       keep_alive=keep_alive,\n\u001b[32m    355\u001b[39m     ).model_dump(exclude_none=\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[32m    356\u001b[39m     stream=stream,\n\u001b[32m    357\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\ollama\\_client.py:180\u001b[39m, in \u001b[36mClient._request\u001b[39m\u001b[34m(self, cls, stream, *args, **kwargs)\u001b[39m\n\u001b[32m    176\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**part)\n\u001b[32m    178\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**\u001b[38;5;28mself\u001b[39m._request_raw(*args, **kwargs).json())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\ollama\\_client.py:124\u001b[39m, in \u001b[36mClient._request_raw\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    122\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m r\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.HTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e.response.text, e.response.status_code) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.ConnectError:\n\u001b[32m    126\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(CONNECTION_ERROR_MESSAGE) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mResponseError\u001b[39m: model requires more system memory (5.6 GiB) than is available (5.5 GiB) (status code: 500)"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import json\n",
    "import re\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "print('importattu')\n",
    "\n",
    "TOPIC = 'Tekoäly'\n",
    "model = 'hf.co/tensorblock/LumiOpen_Llama-Poro-2-8B-Instruct-GGUF'\n",
    "\n",
    "# set up vector database and the retriever:\n",
    "PERSIST_DIR = \"./index/chroma\"\n",
    "COLLECTION = \"artificial_intelligence\"\n",
    "EMBED = \"intfloat/multilingual-e5-small\"\n",
    "emb = HuggingFaceEmbeddings(model_name=EMBED)\n",
    "vs = Chroma(\n",
    "    collection_name=COLLECTION,\n",
    "    persist_directory=PERSIST_DIR,\n",
    "    embedding_function=emb,\n",
    ")\n",
    "retriever = vs.as_retriever(search_kwargs={\"k\": 6})\n",
    "\n",
    "print('vector store ja retriever rdy')\n",
    "\n",
    "# get information from RAG\n",
    "def fetchDocuments(user_query):\n",
    "    docs = retriever.get_relevant_documents(user_query)\n",
    "    context = \"RAG konteksti: \" + \"\\n\\n---\\n\\n\".join(\n",
    "        f\"[LÄHDE: {d.metadata.get('source','tuntematon')}\"\n",
    "        + (f\", sivu {d.metadata.get('page')}\" if d.metadata.get('page') is not None else \"\")\n",
    "        + f\"]\\n{d.page_content[:800]}\"\n",
    "        for d in docs\n",
    "    )\n",
    "    \n",
    "    return \"CONTEXT (faktoihin nojaamiseen, ei käyttäjälle näytettäväksi):\\n\" + context\n",
    "\n",
    "def parse_router_plan(text: str) -> dict:\n",
    "    # poista koodiaidat jos niitä on\n",
    "    t = text.strip()\n",
    "    if t.startswith(\"```\"):\n",
    "        t = re.sub(r\"^```[a-zA-Z]*\\n?\", \"\", t)\n",
    "        t = re.sub(r\"\\n?```$\", \"\", t).strip()\n",
    "\n",
    "    # yritä suoraan\n",
    "    try:\n",
    "        return json.loads(t)\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "\n",
    "    # regex: etsi ensimmäinen {...} lohko\n",
    "    m = re.search(r\"\\{.*\\}\", t, flags=re.S)\n",
    "    if m:\n",
    "        try:\n",
    "            return json.loads(m.group(0))\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "\n",
    "    # fallback\n",
    "    return {\"route\": \"suora\", \"RAG\": \"no\", \"reason\": \"fallback: invalid json\"}\n",
    "\n",
    "print('starting...')\n",
    "\n",
    "# Starting comment\n",
    "beginning_preprompt = f\"opetat oppilasta aiheesta {TOPIC}. Kirjoita alkuviesti, jossa kerrot \\\n",
    "käsiteltävän aiheen ja kysy oppilaalta kysymys, joka selvittää hieman hänen lähtötasoaan (esim. \\\n",
    "Onko tämä aihe sinulle entuudestaan yhtään tuttu?)\"\n",
    "\n",
    "pedagogical_prompt = f\"Käyttäen mahdollista RAG kontekstia alla sekä omia tietojasi, \\\n",
    "ja vastaa näiden pohjalta käyttäjän kysymykseen.\" \n",
    "\n",
    "\n",
    "router_message = 'Sinun tehtäväsi on valita miten ohjelmaa suoritetaan seuraavaksi käyttäjäviestin \\\n",
    "perusteella. Jos viestistä löytyy seuraavia sanoja \"lähde, lähdeaineisto, viimeaikainen, uusi, \\\n",
    "etsiä, etsi, viittaus, viitaten\", tai jos vaikuttaa siltä, että käyttäjä etsii jotain spesifiä \\\n",
    "tietoa, niin valitse vastauspohjaan kohtaan RAG \"yes\" ja aseta route-muuttujaan arvo \"epäsuora\". \\\n",
    "reason kohtaan kirjoita miksi valitsit sen reitin (esim. koska joku edellä mainituista sanoista \\\n",
    "oli käytetty tai viestissä pyydettiin jotain spesifiä tietoa.)\\\n",
    "Anna vastauksesi seuraavan vastauspohjan \\\n",
    "muodossa: {\"route\":\"suora |epäsuora\", \"reason\":\"miksi valitsit tämän reitin\", \"RAG\" : \"yes | no\"} \\\n",
    "Palauta VAIN JSON-yksiö ilman selityksiä, ilman koodiaitoja, ilman ylimääräisiä rivinvaihtoja:'\n",
    "\n",
    "resp = ollama.chat(model=model, messages=[{'role': 'system', 'content': beginning_preprompt}])\n",
    "print('Viola: ', resp['message']['content'])\n",
    "\n",
    "# Conversation loop\n",
    "messages = []\n",
    "while True:\n",
    "    \n",
    "    user = input('Sinä: ')\n",
    "\n",
    "    #exit condition met?\n",
    "    if user in [':exit', ':e', ':quit', ':q', ':lopeta', ':L']:\n",
    "        break\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": user})\n",
    "   \n",
    "    # router - decide what to do next\n",
    "    router_resp = ollama.chat(model=model, messages=[\n",
    "        {'role': 'system', 'content': router_message},\n",
    "        {'role': 'user', 'content': user}\n",
    "    ])\n",
    "\n",
    "    router_content = router_resp[\"message\"][\"content\"]\n",
    "    json_resp = parse_router_plan(router_content)\n",
    "\n",
    "    #decide if tools are needed:\n",
    "    rag_context = \"\"\n",
    "    if json_resp[\"RAG\"] == \"yes\":\n",
    "        # get most relevant documents from the vector database:\n",
    "        rag_context = fetchDocuments(user)\n",
    "    print(rag_context)\n",
    "\n",
    "    #combine pedagogical_prompt and tool-based prompts\n",
    "    system_prompt = pedagogical_prompt + '\\n\\n' + rag_context\n",
    "    print('\\nsystem prompt: ', system_prompt) \n",
    "    # produce the answer:\n",
    "    resp = ollama.chat(model=model, messages=[\n",
    "        {'role': 'system', 'content': system_prompt},\n",
    "        {'role': 'user', 'content': user}\n",
    "    ])\n",
    "    messages.append(resp['message'])\n",
    "    resp_msg = resp['message']['content']\n",
    "    print('vastaus saatu')\n",
    "    # print the answer:\n",
    "    print('\\nViola: ', resp_msg)\n",
    "    messages\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c29c12b-1cda-4f41-acb8-d7ddb3216302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['qwer']\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
